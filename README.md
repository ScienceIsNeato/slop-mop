# ðŸª£ Slop-Mop

<p>
  <a href="https://pypi.org/project/slopmop/"><img src="https://img.shields.io/pypi/v/slopmop.svg" alt="PyPI version"/></a>
  <a href="https://github.com/ScienceIsNeato/slop-mop/actions/workflows/slopmop.yml"><img src="https://github.com/ScienceIsNeato/slop-mop/actions/workflows/slopmop.yml/badge.svg" alt="CI"/></a>
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.10+-blue.svg" alt="Python 3.10+"/></a>
  <a href="https://github.com/ScienceIsNeato/slop-mop/blob/main/LICENSE"><img src="https://img.shields.io/badge/license-Attribution-blue.svg" alt="License"/></a>
</p>

**Quality gates for AI-assisted codebases.** Not a silver bullet â€” just a mop.

<img src="https://raw.githubusercontent.com/ScienceIsNeato/slop-mop/main/assets/heraldic_splash.png" alt="Slop-Mop" width="300" align="right"/>

The code generated by frontier LLMs shares the same features. These days, the majority of those features are positive, but there are still remnants from agent training that result in a set of undesirable outcomes from agentic LLMs â€” especially when working with very small development teams or "vibe coded" projects.

The frontier agents used to generate code were all effectively trained to "accomplish the task at any cost." This makes them great at accomplishing small things with blinders on, but dangerous for the long-term stability of a repository.

The modern development process has turned into a game of whack-a-mole where we're trying to use the output of the agents while also catching LLMs in instances of **shortsightedness**, **deception**, **overconfidence**, and **laziness**. Instead of repeating yourself in chat ad nauseam, let slop-mop do that steering for you.

Not only will slop-mop catch many of the most egregious examples of these behaviors, but it will also tell the agent the precise mistake made and exactly how to fix it. This is a new approach for agentic development â€” turning a "bug" (relentless task accomplishment) into a feature (relentless code quality improvement).

---

## Quick Start

```bash
# Install (once per machine)
pipx install slopmop          # recommended â€” isolated, no dep conflicts
# or: pip install slopmop

# Set up your project
sm init                       # auto-detects languages, writes .sb_config.json

# Run quality gates
sm swab                       # fix what it finds, commit when green
sm scour                      # thorough check before opening a PR
```

`sm init` auto-detects Python, JavaScript, or both and writes a `.sb_config.json` with applicable gates enabled.

---

## The Loop

Development with slop-mop follows a single repeated cycle:

```
sm swab â†’ see what fails â†’ fix it â†’ repeat â†’ commit
```

When a gate fails, the output tells you exactly what to do next:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ¤– AI AGENT ITERATION GUIDANCE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Level: swab                                              â”‚
â”‚ Failed Gate: deceptiveness:py-coverage                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ NEXT STEPS:                                              â”‚
â”‚                                                          â”‚
â”‚ 1. Fix the issue described above                         â”‚
â”‚ 2. Re-check: sm swab -g deceptiveness:py-coverage        â”‚
â”‚ 3. Resume:   sm swab                                     â”‚
â”‚                                                          â”‚
â”‚ Keep iterating until all the slop is mopped.             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This is purpose-built for AI agents. The guidance is machine-readable, the iteration is mechanical, and the agent never has to wonder what to do next. It saves tokens (no flailing), saves CI dollars (catch it locally), and keeps the codebase habitable long-term.

Use `sm status` for a report card of all gates at once.

---

## Why These Categories?

Gates aren't organized by language â€” they're organized by **the failure mode they catch**. These are the four ways LLMs reliably degrade a codebase:

### ðŸ”´ Overconfidence

> *"It compiles, therefore it's correct."*

The LLM generates code that looks right, passes a syntax check, and silently breaks at runtime. These gates verify that the code actually works.

| Gate | What It Does |
|------|--------------|
| `overconfidence:py-tests` | ðŸ§ª Runs pytest â€” code must actually pass its tests |
| `overconfidence:py-static-analysis` | ðŸ” mypy strict â€” types must check out |
| `overconfidence:py-types` | ðŸ”¬ pyright strict â€” second opinion on types |
| `overconfidence:js-tests` | ðŸ§ª Jest test execution |
| `overconfidence:js-types` | ðŸ—ï¸ TypeScript type checking (tsc) |
| `overconfidence:deploy-script-tests` | ðŸš€ Validates deploy scripts |

### ðŸŸ¡ Deceptiveness

> *"Tests pass, therefore the code is tested."*

The LLM writes tests that assert nothing, mock everything, or cover the happy path and call it done. Coverage numbers look great. The code is still broken.

| Gate | What It Does |
|------|--------------|
| `deceptiveness:py-coverage` | ðŸ“Š Whole-repo coverage (80% default threshold) |
| `deceptiveness:py-diff-coverage` | ðŸ“ˆ Coverage on changed lines only (diff-cover) |
| `deceptiveness:bogus-tests` | ðŸ§Ÿ AST analysis for tests that assert nothing |
| `deceptiveness:js-coverage` | ðŸ“Š JavaScript coverage analysis |
| `deceptiveness:js-bogus-tests` | ðŸŽ­ Bogus test detection for JS/TS |

### ðŸŸ  Laziness

> *"It works, therefore it's done."*

The LLM solves the immediate problem and moves on. Formatting is inconsistent, dead code accumulates, complexity creeps upward, and nobody notices until the codebase is incomprehensible.

| Gate | What It Does |
|------|--------------|
| `laziness:py-lint` | ðŸŽ¨ autoflake, black, isort, flake8 (supports auto-fix ðŸ”§) |
| `laziness:js-lint` | ðŸŽ¨ ESLint + Prettier (supports auto-fix ðŸ”§) |
| `laziness:complexity` | ðŸŒ€ Cyclomatic complexity (max rank C) |
| `laziness:dead-code` | ðŸ’€ Dead code detection via vulture (â‰¥80% confidence) |
| `laziness:template-syntax` | ðŸ“„ Jinja2 template validation |
| `laziness:js-frontend` | âš¡ Quick ESLint frontend check |

### ðŸ”µ Myopia

> *"My change is fine. Why would I look at the bigger picture?"*

The LLM has a 200k-token context window and still manages tunnel vision. It duplicates code across files, ignores security implications, and lets functions grow unbounded because it can't see the pattern.

| Gate | What It Does |
|------|--------------|
| `myopia:loc-lock` | ðŸ“ File and function length limits |
| `myopia:source-duplication` | ðŸ“‹ Code clone detection (jscpd) |
| `myopia:string-duplication` | ðŸ”¤ Duplicate string literal detection |
| `myopia:security-scan` | ðŸ” bandit + semgrep + detect-secrets |
| `myopia:security-audit` | ðŸ”’ Full security audit (code + pip-audit) |

### PR Gates

| Gate | What It Does |
|------|--------------|
| `pr:comments` | ðŸ’¬ Checks for unresolved PR review threads |

---

## Levels

Every gate has an intrinsic **level** â€” the point in your workflow where it belongs:

| Level | Command | Gates | When to Use |
|-------|---------|-------|-------------|
| **Swab** | `sm swab` | All overconfidence, deceptiveness, laziness, myopia checks | Before every commit |
| **Scour** | `sm scour` | Everything in swab + PR comments, diff-coverage, full security audit | Before opening or updating a PR |

Scour is a strict superset of swab â€” it runs everything swab does, plus context-dependent gates that need a PR or deeper analysis.

### Aliases

For convenience, named aliases let you run a subset with `-g`:

| Alias | Gates | Purpose |
|-------|-------|---------|
| `quick` | 2 gates â€” lint + security scan | Fast feedback during development |
| `python` | 5 gates â€” Python-specific subset | Language-focused validation |
| `javascript` | 5 gates â€” JS/TS-specific subset | Language-focused validation |
| `quality` | 5 gates â€” complexity, duplication, loc-lock | Code quality only |
| `security` | 1 gate â€” full security audit | Security-focused validation |

JS gates auto-skip when no JavaScript is detected.

### Time Budget (Preview)

Short on time? Use `--swabbing-time` to set a budget in seconds. Gates are
ordered by historical runtime and skipped once the budget would be exceeded:

```bash
sm swab --swabbing-time 30    # only run gates that fit in ~30 seconds
sm scour --swabbing-time 120  # thorough pass, but cap at 2 minutes
```

> **Note:** `--swabbing-time` is a preview feature â€” the flag is accepted but
> budget enforcement is not yet active. Full implementation is coming in a
> future release.

---

## Getting Started: The Remediation Path

Most projects won't pass all gates on day one. That's expected. Here's the ramp:

### 1. Initialize

```bash
sm init                       # auto-detects everything, writes .sb_config.json
```

### 2. See Where You Stand

```bash
sm swab                       # run swab-level gates, see what fails
sm status                     # full report card
```

### 3. Disable What You're Not Ready For

```bash
sm config --disable laziness:complexity        # too many complex functions right now
sm config --disable deceptiveness:py-coverage  # coverage is at 30%, not 80%
sm swab                                        # get the rest green first
```

### 4. Fix Everything That's Left

Iterate: run `sm swab`, fix a failure, run again. The iteration guidance tells you exactly what to do after each failure.

### 5. Install Hooks

```bash
sm commit-hooks install           # pre-commit hook runs sm swab
sm commit-hooks status            # verify hooks are installed
```

Now every `git commit` runs slop-mop. Failed gates block the commit.

### 6. Re-enable Gates Over Time

```bash
sm config --enable laziness:complexity         # refactored enough, turn it on
sm config --enable deceptiveness:py-coverage   # coverage is at 75%, set threshold to 70
```

### 7. Let Agents Vibe-Code

With hooks in place, agents can write code freely. Slop-mop catches the slop before it reaches the repo. This saves tokens (no back-and-forth debugging), saves CI money (catch it locally), and keeps the codebase survivable long-term.

---

## Configuration

```bash
sm config --show              # show all gates and their status
sm config --enable <gate>     # enable a disabled gate
sm config --disable <gate>    # disable a gate
sm config --json <file>       # bulk update from JSON
```

### Include / Exclude Directories

```bash
sm config --exclude-dir myopia:generated       # skip generated code
sm config --include-dir overconfidence:src      # only check src/
```

- `include_dirs`: whitelist â€” only these dirs are scanned
- `exclude_dirs`: blacklist â€” always skipped, takes precedence

### .sb_config.json

Edit directly for per-gate configuration:

```json
{
  "version": "1.0",
  "python": {
    "gates": {
      "coverage": { "threshold": 80 },
      "tests": { "test_dirs": ["tests"] }
    }
  },
  "quality": {
    "exclude_dirs": ["generated", "vendor"]
  }
}
```

---

## CI Integration

### GitHub Actions

```yaml
name: slop-mop
on:
  pull_request:
    branches: [main]
  push:
    branches: [main]

jobs:
  quality-gates:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install slopmop
      - run: sm swab
      - if: github.event_name == 'pull_request'
        env:
          GH_TOKEN: ${{ github.token }}
        run: sm scour
```

### Check CI Status Locally

```bash
sm ci               # current PR
sm ci 42             # specific PR
sm ci --watch        # poll until CI completes
```

---

## Architecture

Slop-mop installs as a normal package and is configured per-project via `.sb_config.json`. The `sm` command is on your PATH once and works in any repo.

**Tool resolution order** â€” sm uses your project's tools when available:
1. `<project_root>/venv/bin/<tool>` or `.venv/bin/<tool>` â€” project-local venv (highest priority)
2. `$VIRTUAL_ENV/bin/<tool>` â€” currently activated venv
3. System PATH â€” sm's own bundled tools (via pipx)

This means if your project has its own `pytest` (with plugins like `pytest-django`), sm uses it. Otherwise, sm falls back to its own.

**Submodule alternative**: If you need strict version pinning, add `slop-mop` as a git submodule and invoke `python -m slopmop.sm` directly. Supported but not recommended for most projects.

---

## Development

```bash
# Working on slop-mop itself
pip install -e .
sm scour --self               # dogfooding â€” sm validates its own code
pytest
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for the process of adding new gates.

---

## Further Reading

> ðŸ“– [A Hand for Daenerys: Why Tyrion Is Missing from Your Vibe-Coding Council](https://scienceisneato.substack.com/p/a-hand-for-daenerys-why-tyrion-is) â€” the article that started this project.

---

## License

[Slop-Mop Attribution License v1.0](LICENSE) â€” free to use, modify, and redistribute with attribution.

P.S. Other than this line in the readme and a few scattered lines here and there, nothing in this project was written by a human. It is, for better or worse, the result of living under the slop-mop regime.
